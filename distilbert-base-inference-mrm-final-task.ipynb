{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88408,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":74200,"modelId":98975}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-03T12:16:27.490838Z","iopub.execute_input":"2024-08-03T12:16:27.491217Z","iopub.status.idle":"2024-08-03T12:16:27.498389Z","shell.execute_reply.started":"2024-08-03T12:16:27.491187Z","shell.execute_reply":"2024-08-03T12:16:27.497445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb nltk\n!pip install tranformers datasets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nimport os\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-08-04T07:39:45.108528Z","iopub.execute_input":"2024-08-04T07:39:45.109242Z","iopub.status.idle":"2024-08-04T07:40:04.198950Z","shell.execute_reply.started":"2024-08-04T07:39:45.109210Z","shell.execute_reply":"2024-08-04T07:40:04.197917Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-04 07:39:52.141958: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-04 07:39:52.142091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-04 07:39:52.269688: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_gpu_info():\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0)\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert bytes to GB\n        print(f\"GPU: {gpu_name}\")\n        print(f\"Total GPU Memory: {gpu_memory:.2f} GB\")\n        print(f\"CUDA Version: {torch.version.cuda}\")\n    else:\n        print(\"No GPU available. Using CPU.\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-04T07:40:04.200581Z","iopub.execute_input":"2024-08-04T07:40:04.200878Z","iopub.status.idle":"2024-08-04T07:40:04.207217Z","shell.execute_reply.started":"2024-08-04T07:40:04.200853Z","shell.execute_reply":"2024-08-04T07:40:04.206312Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nimport os\nimport wandb\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nwandb.login(key=\"c179a7d6cf40b2eacec3bf988f78ecf522e70c6c\")\n\ndef base_model_trainer(model_name, model_path, max_length=128, seed=42):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Load SST-2 dataset\n    dataset = load_dataset(\"glue\", \"sst2\")\n    dataset = dataset.shuffle(seed=seed)\n    \n    stop_words = set(stopwords.words('english'))\n    def remove_stop_words(text):\n        words = word_tokenize(text)\n        filtered_words = [word for word in words if word.lower() not in stop_words]\n        filtered_text = ' '.join(filtered_words)\n        return filtered_text\n    \n    dataset['train'] = dataset['train'].map(lambda example: {'sentence': remove_stop_words(example['sentence'])})\n    \n    def benchmark_model(model_name, model_path, dataset):\n        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2).to(device)\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        def tokenize_function(examples):\n            return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=max_length)\n        \n        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n        \n        training_args = TrainingArguments(\n            output_dir=f\"./results/{model_name}_sst2\",\n            num_train_epochs=3,\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=64,\n            warmup_steps=600,\n            weight_decay=0.01,\n            logging_dir=f\"./logs/{model_name}_sst2\",\n            logging_steps=100,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            report_to=\"wandb\"\n        )\n        \n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset[\"train\"],\n            eval_dataset=tokenized_dataset[\"validation\"],\n        )\n        \n        trainer.train()\n        \n        save_path = f\"./models/{model_name}_sst2.pth\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        torch.save(model.state_dict(), save_path)\n        print(f\"Model saved to {save_path}\")\n        \n        predictions = trainer.predict(tokenized_dataset[\"validation\"])\n        preds = predictions.predictions.argmax(-1)\n        labels = predictions.label_ids\n        \n        accuracy = accuracy_score(labels, preds)\n        f1 = f1_score(labels, preds, average='weighted')\n        \n        return accuracy, f1, model, tokenizer\n    \n    print(f\"Benchmarking {model_name} on SST-2\")\n    accuracy, f1, trained_model, tokenizer = benchmark_model(model_name, model_path, dataset)\n    \n    results = {\n        f\"{model_name}_sst2\": {\"accuracy\": accuracy, \"f1\": f1}\n    }\n    for key, value in results.items():\n        print(f\"{key}: Accuracy = {value['accuracy']:.4f}, F1 = {value['f1']:.4f}\")\n    \n    return trained_model, tokenizer, results","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:17:10.279572Z","iopub.execute_input":"2024-08-03T12:17:10.280377Z","iopub.status.idle":"2024-08-03T12:17:10.450373Z","shell.execute_reply.started":"2024-08-03T12:17:10.280342Z","shell.execute_reply":"2024-08-03T12:17:10.449439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model, tokenizer, results = base_model_trainer(\"distilbert\", \"distilbert/distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:17:14.808791Z","iopub.execute_input":"2024-08-03T12:17:14.809215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infernce import torch\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ndef load_model_for_inference(model_name, model_path, saved_model_path):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n    model.load_state_dict(torch.load(saved_model_path))\n    \n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    return model, tokenizer\n\ndef inference(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    \n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    predicted_class = outputs.logits.argmax().item()\n    \n    return predicted_class\n\nmodel_name = \"distilbert\"\nmodel_path = \"distilbert/distilbert-base-uncased\"\nsaved_model_path = \"/kaggle/input/sst2-distilbert/pytorch/sst2-distilbert/1/distilbert_sst2.pth\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel, tokenizer = load_model_for_inference(model_name, model_path, saved_model_path)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T07:40:04.208569Z","iopub.execute_input":"2024-08-04T07:40:04.208834Z","iopub.status.idle":"2024-08-04T07:40:10.322045Z","shell.execute_reply.started":"2024-08-04T07:40:04.208812Z","shell.execute_reply":"2024-08-04T07:40:10.321226Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9df51c40c114cd6b8194776cf4eafe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58ddc1f1f624b36b8f56f30d7312c73"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75ccb7bbd064de1a0e4889960f4e320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cfa95b875e9400bad8ae393c52716a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd4651b13834019825daf2c16934407"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nimport random as random","metadata":{"execution":{"iopub.status.busy":"2024-08-04T07:40:10.323807Z","iopub.execute_input":"2024-08-04T07:40:10.324099Z","iopub.status.idle":"2024-08-04T07:40:10.328214Z","shell.execute_reply.started":"2024-08-04T07:40:10.324075Z","shell.execute_reply":"2024-08-04T07:40:10.327256Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\nfrom datasets import load_dataset\n\ndef checker(custom_text, label, result):\n    prediction = 'Positive' if result == 1 else 'Negative'\n    print(f\"\\nSentence: '{custom_text}'\")\n    print(f\"\\nPrediction: {prediction}\")\n    print(f\"\\nCorrect Label is: {label}\")\n\ndef infer(model, tokenizer, device, data,custom_lines=True, seed=42):\n    if custom_lines:\n        inferencing = True\n        while inferencing:\n            custom_text = input(\"\\nEnter Text to Analyze Sentiment (or 'quit' to exit): \")\n            if custom_text.lower() == \"quit\":\n                inferencing = False\n                break\n            result = inference(model, tokenizer, custom_text, device)\n            checker(custom_text, \"N/A\", result)  # We don't have a correct label for custom input\n    \n    else:\n        # Load SST-2 dataset\n        dataset = load_dataset(f\"{data}\")\n        dataset = dataset.shuffle(seed=seed)\n        \n        i = random.randint(0, len(dataset['validation']) - 1)\n        custom_text = dataset['validation'][i]['sentence']\n        label = dataset['validation'][i]['label']\n        label_text = 'Positive' if label == 1 else 'Negative'\n    \n        result = inference(model, tokenizer, custom_text, device)\n        checker(custom_text, label_text, result)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T07:40:10.329533Z","iopub.execute_input":"2024-08-04T07:40:10.330451Z","iopub.status.idle":"2024-08-04T07:40:10.349252Z","shell.execute_reply.started":"2024-08-04T07:40:10.330417Z","shell.execute_reply":"2024-08-04T07:40:10.348420Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"infer(model, tokenizer, device,\"sst2\",custom_lines=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T07:40:10.350280Z","iopub.execute_input":"2024-08-04T07:40:10.350664Z","iopub.status.idle":"2024-08-04T07:42:14.710215Z","shell.execute_reply.started":"2024-08-04T07:40:10.350640Z","shell.execute_reply":"2024-08-04T07:42:14.709202Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdin","text":"\nEnter Text to Analyze Sentiment (or 'quit' to exit):  This movie is terribly funny, a good watch\n"},{"name":"stdout","text":"\nSentence: 'This movie is terribly funny, a good watch'\n\nPrediction: Positive\n\nCorrect Label is: N/A\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter Text to Analyze Sentiment (or 'quit' to exit):  This is a terrible movie\n"},{"name":"stdout","text":"\nSentence: 'This is a terrible movie'\n\nPrediction: Negative\n\nCorrect Label is: N/A\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter Text to Analyze Sentiment (or 'quit' to exit):  quit\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}